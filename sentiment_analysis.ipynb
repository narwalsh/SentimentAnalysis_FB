{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sentiment_Analysis\n",
    "# by Patrick Walsh\n",
    "# Januray 29, 2016\n",
    "\n",
    "\n",
    "\"\"\"The following begins a modification of the 'Sentiment_Analysis' module.  The goal is to introduce a degree\n",
    "of control over the number of comments processed for a given post, such as intoducing random sampling when the number\n",
    "of comments is over a certain theshold, and implementing a maximum number of comments to be processed.\n",
    "\n",
    "Upon further review, we've noticed that comments are ordered by the number of likes that each receives.  \n",
    "Rather than sampling randomly, we've decided to take the first 'x' comments \n",
    "where x is an integer such that x % 25 = 0\n",
    "\n",
    "Note: it takes approximately    34 seconds to process a post with 100 comments\n",
    "                                20 seconds for 50\n",
    "                                10 seconds for 25\"\"\"\n",
    "\n",
    "def avg_sentiment(url, pages):\n",
    "    \"\"\"\n",
    "    accepts a post url and the maximum number of pages allowed (upt o 25 comments per page)\n",
    "    and returns metrics sumarizing the sentiment of comments in the form of a pandas dataframe\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from textblob import TextBlob\n",
    "    import datetime\n",
    "    import json\n",
    "    import string\n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    import numpy\n",
    "\n",
    "    #print \"Intitalizing\"\n",
    "    number_of_comments = 0\n",
    "    avg_polarity = 0\n",
    "    avg_subjectivity = 0\n",
    "    avg_words = 0\n",
    "    avg_sentences = 0\n",
    "    count = 0\n",
    "    page_count = 0\n",
    "    \n",
    "    access_token = 'CAAAACuIpepUBACIH5lgj4qpAZA30qSpTZCvIAsDGnMrNYg5xgM8nC5iUsg56khHgjzWMO6MBSZALGDLzRZANvfrTPmXFuRZBcMKxBR8GOmpE6WmEQDzbNZBrYvXx5x1rscW7pnb3ROeYw82T8OtLpVGuUrdped1VB0AWGJJAziDf1eujZAv3StfRHIKZCJ39u6MZD'\n",
    "    graph_url = 'https://graph.facebook.com/comments/?ids=%s&access_token=%s&summary=1' %(url, access_token)\n",
    "    results = requests.get(graph_url).json() #pings facebook graph api returning a page of up to 25 comments\n",
    "    \n",
    "\n",
    "    # initial ping: necessary b/c formatting changes on paginated results\n",
    "    try:\n",
    "        num_comments = len(results[url]['comments']['data'])\n",
    "    except:\n",
    "        num_comments = 0\n",
    "        \n",
    "    if num_comments > 0:\n",
    "        number_of_comments = number_of_comments + num_comments\n",
    "        total_comments = results[url]['comments']['summary']['total_count']\n",
    "        page_count = 1\n",
    "        \n",
    "        # if comments --> start analysis on first page\n",
    "        for rep in range(num_comments):\n",
    "            text = results[url]['comments']['data'][rep]['message'] # note this is different than the call that must be made for subsequent pages\n",
    "            text = TextBlob(text)\n",
    "            text = text.correct()\n",
    "            \n",
    "            comment_polarity = text.sentiment.polarity\n",
    "            comment_subjectivity = text.sentiment.subjectivity\n",
    "            words = len(text.words)\n",
    "            sentences = len(text.sentences)\n",
    "\n",
    "            avg_polarity += comment_polarity\n",
    "            avg_subjectivity += comment_subjectivity\n",
    "            avg_words += words\n",
    "            avg_sentences += sentences\n",
    "            count = count +1\n",
    "        \n",
    "        # proceed to subsequent pages\n",
    "        try:\n",
    "            graph_url = results[url]['comments']['paging']['next']\n",
    "        except:\n",
    "            graph_url = None    \n",
    "\n",
    "        while (graph_url != None) and (page_count < pages):\n",
    "            results = requests.get(graph_url).json() #pings facebook graph api returning a page of up to 25 comments\n",
    "\n",
    "            try:\n",
    "                num_comments = len(results['data'])\n",
    "\n",
    "            except:\n",
    "                num_comments = 0\n",
    "\n",
    "            if num_comments > 0:\n",
    "                page_count = page_count + 1\n",
    "                number_of_comments = number_of_comments + num_comments\n",
    "                for rep in range(num_comments):\n",
    "                    text = results['data'][rep]['message'] # note this is different than the call that must be made for first page\n",
    "                    text = TextBlob(text)\n",
    "                    text = text.correct()\n",
    "\n",
    "                    comment_polarity = text.sentiment.polarity\n",
    "                    comment_subjectivity = text.sentiment.subjectivity\n",
    "                    words = len(text.words)\n",
    "                    sentences = len(text.sentences)\n",
    "\n",
    "                    avg_polarity += comment_polarity\n",
    "                    avg_subjectivity += comment_subjectivity\n",
    "                    avg_words += words\n",
    "                    avg_sentences += sentences\n",
    "                    count = count + 1\n",
    "\n",
    "                try:\n",
    "                    graph_url = results['paging']['next'] # try to retrieve next page    \n",
    "                except:\n",
    "                    graph_url = None   \n",
    "            else:\n",
    "                number_of_comments = number_of_comments + 0\n",
    "\n",
    "\n",
    "        \n",
    "    else:\n",
    "        total_comments = 0\n",
    "        number_of_comments = 0\n",
    "        avg_polarity = \" \"\n",
    "        avg_subjectivity = \" \" \n",
    "        avg_words = \" \"\n",
    "        avg_sentences = \" \" \n",
    "        \n",
    "    \n",
    "    # post processing\n",
    "    comments_processed = count\n",
    "    \n",
    "    if comments_processed > 0:\n",
    "        avg_polarity = avg_polarity / count\n",
    "        avg_subjectivity = avg_subjectivity / count\n",
    "        avg_words = avg_words / count\n",
    "        avg_sentences = avg_sentences / count\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(columns = ('url', 'total_comments','comments_processed', 'avg_words','avg_sentences','avg_polarity','avg_subjectivity'))\n",
    "    df.loc[0] = url, total_comments, comments_processed, avg_words, avg_sentences, avg_polarity, avg_subjectivity\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time:  10.5688049793  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "before = time.time()\n",
    "avg_sentiment(url = 'http://www.huffingtonpost.com/entry/ben-carson-weird-answers_us_56aae557e4b077d4fe8d91f1?', pages = 1)\n",
    "after = time.time()\n",
    "run_time = after - before\n",
    "print \"Run time: \", run_time,\" seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
